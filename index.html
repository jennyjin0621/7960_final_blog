<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
		background-color: #f5f9ff;
	}

	/* Hide both math displays initially, will display based on JS detection */
  .mathjax-mobile, .mathml-non-mobile { display: none; }

  /* Show the MathML content by default on non-mobile devices */
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

	.content-margin-container {
		display: flex;
		width: 100%;
		justify-content: left;
		align-items: center;
	}
	.main-content-block {
		width: 70%;
    max-width: 1100px;
		background-color: #fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 8px 8px 8px 8px;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
	}
	.margin-left-block {
			font-size: 14px;
			width: 15%;
			max-width: 130px;
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
			padding: 5px;
	}
	.margin-right-block {
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
			font-size: 14px;
			width: 25%;
			max-width: 256px;
			position: relative;
			text-align: left;
			padding: 10px;
			color: #555;
			line-height: 1.35;
	}

	img {
			max-width: 100%;
			height: auto;
			display: block;
			margin: auto;
	}
	.my-video {
			max-width: 100%;
			height: auto;
			display: block;
			margin: auto;
	}
  .vid-mobile, .vid-non-mobile { display: none; }
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

	a:link,a:visited {
		color: #0e7862;
		text-decoration: none;
	}
	a:hover {
		color: #24b597;
	}

	h1 {
		font-size: 20px;
		margin-top: 10px;
		margin-bottom: 10px;
	}

	h2 {
		font-size: 18px;
		margin-top: 10px;
		margin-bottom: 10px;
	}

	h3 {
		font-size: 17px;
		margin-top: 8px;
		margin-bottom: 4px;
	}

	h4 {
		font-size: 16px;
		margin-top: 4px;
		margin-bottom: 4px;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 70%;
    max-width: calc(100% - 290px);
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}
	table.paper-code-tab {
	    flex-shrink: 0;
	    margin-left: 8px;
	    margin-top: 8px;
	    padding: 0px 0px 0px 8px;
	    width: 290px;
	    height: 150px;
	}

	.layered-paper {
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35),
		        5px 5px 0 0px #fff,
		        5px 5px 1px 1px rgba(0,0,0,0.35),
		        10px 10px 0 0px #fff,
		        10px 10px 1px 1px rgba(0,0,0,0.35);
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	hr {
    height: 1px;
    border: none;
    background-color: #DDD;
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		font-family: Courier;
		font-size: 18px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	.fade-in-inline {
		position: absolute;
		text-align: center;
		margin: auto;
		-webkit-mask-image: linear-gradient(to right,
																			transparent 0%,
																			transparent 40%,
																			black 50%,
																			black 90%,
																			transparent 100%);
		mask-image: linear-gradient(to right,
																transparent 0%,
																black 50%,
																black 90%,
																transparent 100%);
		-webkit-mask-size: 8000% 100%;
		mask-size: 8000% 100%;
		animation-name: sweepMask;
		animation-duration: 4s;
		animation-iteration-count: infinite;
		animation-timing-function: linear;
		animation-delay: -1s;
	}

	.fade-in2-inline {
			animation-delay: 1s;
	}

	.inline-div {
			position: relative;
	    display: inline-block;
	    vertical-align: top;
	    width: 50px;
	}

  .highlight-box {
      background-color: #e8f3ff;
      border-left: 3px solid #0e7862;
      padding: 10px 14px;
      border-radius: 6px;
      margin: 16px 0;
  }

</style>

	  <title>Decoding Attention: A Study of Distributional Assumptions in Time Series Transformers</title>
      <meta property="og:title" content="Decoding Attention: A Study of Distributional Assumptions in Time Series Transformers" />
			<meta charset="UTF-8">
  </head>
  

  <body>

		<!-- HEADER -->
<div class="content-margin-container">
  <div class="margin-left-block">
  </div>
  <div class="main-content-block">
    <table class="header" align="left">
      <tr>
        <td colspan="4">
          <span style="font-size: 32px; font-family: 'Courier New', Courier, monospace;">
            Decoding Attention: A Study of Distributional Assumptions in Time Series Transformers
          </span>
        </td>
      </tr>
      <tr>
        <td align="left">
          <span style="font-size:17px">Summer Zhou</span>
        </td>
        <td align="left">
          <span style="font-size:17px">Kevin Wang</span>
        </td>
        <td align="left">
          <span style="font-size:17px">Jenny Jin</span>
        </td>
      </tr>
      <tr>
        <td colspan="4" align="left">
          <span style="font-size:18px">Final project for 6.7960, MIT</span>
        </td>
      </tr>
    </table>
  </div>
  <div class="margin-right-block">
  </div>
</div>


    <!-- OUTLINE + HERO IMAGE -->
    <div class="content-margin-container" id="top">
      <div class="margin-left-block">
        <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
          <b style="font-size:16px">Outline</b><br><br>
          <a href="#intro">Introduction</a><br><br>
          <a href="#background">Background and Related Work</a><br><br>
          <a href="#methods">Methods</a><br><br>
          <a href="#results">Results</a><br><br>
          <a href="#conclusion">Conclusion</a><br><br>
        </div>
      </div>

      <!-- main white column with hero image -->
      <div class="main-content-block">
        <img src="./images/multi_attention.png"
            width="650px"
            style="display:block; margin:15px auto 25px auto;"/>
      </div>

      <!-- right margin caption -->
      <div class="margin-right-block" style="transform: translate(0%, -20%);">
        <b>Figure 1.</b> Overview attention patterns on a multi-seasonal time series, illustrating how Transformers learn grid-like structure from periodic data.
      </div>
    </div>


    <!-- INTRODUCTION -->
    <div class="content-margin-container" id="intro">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Introduction</h1>

            <p>
              Transformers have rapidly become a go-to architecture for time series modeling, yet we still lack a clear understanding of what their attention mechanisms actually learn on temporal data. In natural language, attention weights can sometimes be linked to meaningful word dependencies, but in time series tasks the story is more complicated. Forecasting models often assume that attention will highlight important timesteps and that these weights reveal the underlying temporal structure driving predictions. Whether this assumption holds—and under what <em>distributional </em>conditions—remains an open question.
            </p>

            <p>
              This project investigates a fundamental issue: for which statistical structures in a time series does Transformer attention behave in a meaningful, interpretable way, and when does it become misleading or degenerate? To answer this, we analyze learned attention using attention heatmaps, supported by quantitative measures such as layer entropy and forecasting error (MSE). Our hypothesis is that self-attention produces coherent, interpretable patterns only when the data contain stable lag structure (e.g., periodicity, smooth trends, low-noise stationarity). In contrast, we expect attention to collapse into recency bias, high-entropy diffusion, or noise-amplifying behavior when the process contains jumps, heavy-tailed shocks, volatility clustering, or other forms of non-stationarity.
            <p>
              To test this hypothesis, we train identical Transformer models on synthetic time series generated from processes with known structure (e.g., jump diffusion, trend breaks) as well as on real financial data (e.g., S&P 500 daily prices). This controlled setup allows us to ask precise questions:
            </p>

			<ul>
				<li>
          <b>How do attention patterns differ when the underlying process exhibits stable lag structure versus jumps, volatility clusters, or regime shifts?</b>
				</li>
				<li>
          <b> Do we observe meaningful, low-entropy, localized attention around true structural events, or do the maps become diffuse or dominated by recency?</b>
        </li>
			</ul>

		    </div>
		    <div class="margin-right-block">
					<b>Big picture</b><br><br>
					Transformers work well on time series in practice, but it’s not clear
					<i>what</i> their attention is actually keying off of.<br><br>

					Our central question is: do those attention weights line up with the real
					statistical structure of the series, or just with local patterns the model
					finds convenient during training?
		    </div>
		</div>

        <!-- BACKGROUND AND RELATED WORK -->
		<div class="content-margin-container" id="background">
			<div class="margin-left-block">
			</div>
			<div class="main-content-block">
				<h1>Background and Related Work</h1>

				<p>There are more transformers being used in time-series forecasting, but whether their attention mechanisms <em>actually learn meaningful temporal structure</em> is less explored. Several recent works directly motivate our study.		</p>

				<b>Transformers Often Fail to Capture Temporal Dependencies</b>

				<p>Zeng et al. (2022) showed that simple linear models (e.g., DLinear) outperform many Transformer variants when trained fairly, arguing that self-attention often fails to discover lag dependencies and instead benefits from dataset or training biases. This raises the key question driving our work: Does attention succeed under certain time-series distributions and fail under others?
				</p>

				<b>Sparse Attention as an Inductive Bias</b>

				<p>
          Informer (Zhou et al., 2021) introduced ProbSparse attention, which selects dominant queries and enforces sparsity—implicitly assuming that only a few past lags matter. This matches classical time-series intuition and may expose temporal structure more clearly than dense attention. Our study inspired by this and set regularization terms carefully for training our Transformer.
				</p>

				<b>Attention Behavior Depends on Statistical Properties of the Data</b>

				<p>
          Recent architectures also highlight challenges between standard self-attention and real temporal dynamics. For example, the Non-Stationary Transformer (Liu et al., 2022) shows that common normalization steps can inadvertently remove distributional shifts, leading to diffuse, high-entropy attention maps, while Autoformer (Wu et al., 2021) demonstrates that self-attention often fails to capture periodic structure, requiring an alternative auto-correlation mechanism. Together, these findings indicate that properties such as stationarity, volatility clustering, jumps, and regime changes can fundamentally shape how attention behaves.
				</p>

				<b>Why Our Work Fills a Gap</b>

				<p>
          Most prior work on time-series Transformers focuses on forecasting accuracy or architectural improvements, rather than examining how attention behaves under different statistical regimes. Very few studies ask whether attention meaningfully reflects temporal structure when the underlying data come with properties hard to capture. 
				</p>

        <p>
          Our project fills this gap by constructing a controlled univariate synthetic benchmark spanning these distributions and training both dense and sparse (ProbSparse) Transformers under identical conditions. We analyze attention using entropy, sparsity, and pattern-detection metrics to quantify how statistical properties shape learned dependencies. This allows us to directly connect attention patterns to data-generating processes, revealing when attention is informative and provide guidance on further data preprocessing on real data.
        </p>
			</div>
			<div class="margin-right-block">
				<b>Key papers we build on</b><br>
				• Zeng et al. (2022) – DLinear vs. Transformers.<br>
				• Zhou et al. (2021) – Informer with ProbSparse attention.<br>
				• Wu et al. (2021) – Autoformer for trend/seasonality.<br>
				• Liu et al. (2022) – Non-Stationary Transformer.<br><br>

				<b>Our twist</b><br>
				• Controlled synthetic distributions (AR, jump diffusion, seasonality, etc.).<br>
				• Directly compare dense vs. sparse attention on the <i>same</i> processes.<br>
				• Tie attention maps back to known ground-truth structure.
			</div>
		</div>



        <!-- METHODS: TEXT -->
        <div class="content-margin-container" id="methods">
          <div class="margin-left-block">
          </div>

          <div class="main-content-block">
            <h1>Methods</h1>

            <h2>Overview of Analytical Pipeline</h2>

            <p>
              Our methodology is designed to isolate how the distributional structure of a time series influences attention behavior in Transformers. The analysis follows a fixed pipeline across all experiments:
            </p>
            <ol>
              <li><b>Generate synthetic time series</b>, each constructed to contain one dominant statistical property (heavy tails, jumps, trend shifts, seasonality, multi-seasonality).</li>
              <li><b>Normalize and split</b> each series into train/validation/test sets.</li>
              <li><b>Train an identical 2-layer encoder-only Transformer</b> across all generators with fixed hyperparameters.</li>
              <li><b>Extract raw attention maps</b> from each layer and head using a custom forward pass.</li>
              <li><b>Compute metrics</b> (entropy, sparsity, lag-pattern concentration, head specialization).</li>
              <li><b>Visualize</b> attention heatmaps and entropy curves.</li>
              <li><b>Repeat the same analysis on real datasets </b> (S&amp;P 500, Solar) to examine whether synthetic insights generalize.</li>
            </ol>
            <p>This controlled, distribution-aligned setup allows us to attribute differences in attention behavior to the properties of the data rather than to model or training variability.</p>

            <h2>Model Architecture</h2>

            <!-- image sits directly under the H2 -->
            <img src="./images/model.png"
                width="450px"
                style="display:block; margin:15px auto 25px auto;"/>

            <p>
              We use a lightweight 2-layer encoder-only Transformer designed expressly for interpretability. The model takes a window of L=100 past observations and predicts H=20 future values. Each input window is mapped to a d_model = 64 ​dimensional embedding to heads to specialize while maintaining attention pattern readability. Two Transformer encoder layers, each containing four head self-attention, feedforward blocks, residual connections, and layer normalization. The multi-head structure allows different heads to specialize in different types of temporal dependencies. The final hidden states are linearly projected to produce H=20 future values. We implement a custom forward pass to extract raw (batch,head,L,L) attention matrices from every layer, enabling entropy, sparsity, and structural analysis of attention behavior. All series are normalized using training-set statistics for comparability across generators, and all models are trained for a fixed 50-epoch schedule to ensure no training variability.
            </p>

            <h2>Synthetic Data Generation</h2>

              <p>
                To study how individual statistical properties affect attention, we generate univariate time series where <b> each generator embodies exactly one dominant regime.</b> Table 1 summarizes the generators and their key parameters.
              </p>
              </div>

              <!-- Right margin for this whole block (including Figure 2) -->
              <div class="margin-right-block">
                <b>Figure 2.</b><br>
                Model architecture illustration for the 2-layer encoder-only Transformer.
              </div>
              </div>


              <!-- TABLE 1: Synthetic Data Generators and Parameters -->
              <div class="content-margin-container">
                <div class="margin-left-block">
                </div>

                <div class="main-content-block">
                  <table style="width:100%; border-collapse:collapse; font-size:14px;">
                    <thead>
                      <tr>
                        <th style="border:1px solid #ccc; padding:6px 8px; text-align:left;">Generator</th>
                        <th style="border:1px solid #ccc; padding:6px 8px; text-align:left;">Statistical Property</th>
                        <th style="border:1px solid #ccc; padding:6px 8px; text-align:left;">Key Parameters</th>
                      </tr>
                    </thead>
                    <tbody>
                      <tr>
                        <td style="border:1px solid #ccc; padding:6px 8px;">HeavyTailedAR1</td>
                        <td style="border:1px solid #ccc; padding:6px 8px;">Persistence with rare extreme shocks</td>
                        <td style="border:1px solid #ccc; padding:6px 8px;">&phi; = 0.7; Student-t(df = 3) noise</td>
                      </tr>
                      <tr>
                        <td style="border:1px solid #ccc; padding:6px 8px;">JumpDiffusion</td>
                        <td style="border:1px solid #ccc; padding:6px 8px;">Smooth diffusion with discontinuous jumps</td>
                        <td style="border:1px solid #ccc; padding:6px 8px;">&sigma; = 1.0; jump probability = 0.05; jump size = 5</td>
                      </tr>
                      <tr>
                        <td style="border:1px solid #ccc; padding:6px 8px;">TrendBreaks</td>
                        <td style="border:1px solid #ccc; padding:6px 8px;">Piecewise-linear segments with regime shifts</td>
                        <td style="border:1px solid #ccc; padding:6px 8px;">Slopes = [0.1, −0.1, 0.05]; breakpoints at t = 300, 700</td>
                      </tr>
                      <tr>
                        <td style="border:1px solid #ccc; padding:6px 8px;">SeasonTrendOutliers</td>
                        <td style="border:1px solid #ccc; padding:6px 8px;">Seasonality + trend + sparse outliers</td>
                        <td style="border:1px solid #ccc; padding:6px 8px;">Trend = 0.01t; amplitude = 2; frequency = 0.1; outliers = N(0, 5)</td>
                      </tr>
                      <tr>
                        <td style="border:1px solid #ccc; padding:6px 8px;">MultiSeasonality</td>
                        <td style="border:1px solid #ccc; padding:6px 8px;">Multiple overlapping periodic cycles</td>
                        <td style="border:1px solid #ccc; padding:6px 8px;">Amplitudes = [3, 1.5]; frequencies = [0.1, 0.05]</td>
                      </tr>
                    </tbody>
                  </table>
                </div>

                <div class="margin-right-block">
                  <b>Table 1.</b><br>
                  Synthetic data generators and parameters used in our study.
                </div>
              </div>

              <!-- TEXT ABOUT SERIES LENGTH + WHAT ATTENTION REFLECTS -->
              <div class="content-margin-container">
                <div class="margin-left-block">
                </div>
                <div class="main-content-block">
                  <p> 
                    Each series has length 2000 and is split into 70% train, 15% validation, 15% test. All series are standardized using training-set statistics to prevent scale differences from influencing attention.
                  </p>
                  <p>These synthetic datasets enable controlled tests of whether attention maps reflect: Lag structure, Volatility clustering or uncertainty, Periodic recurrence, Regime segmentation (trend breaks). </p>
                </div>
                <div class="margin-right-block">
                </div>
              </div>

              <!-- METHODS: SYNTHETIC FIGURE (Figure 3) -->
              <div class="content-margin-container">
                <div class="margin-left-block">
                </div>
                <div class="main-content-block">
                  <img src="./images/synthetic_data.png"
                      width="950px"
                      style="display:block; margin:15px auto 25px auto;"/>
                </div>
                <div class="margin-right-block">
                  <b>Figure 3.</b><br>
                  Synthetic datasets used in our study: heavy-tailed AR(1), jump diffusion, trend breaks, seasonal with trend and outliers, and multi-seasonal time series.
                </div>
              </div>


        <!-- EVALUATION METRICS -->
        <div class="content-margin-container">
          <div class="margin-left-block">
          </div>
          <div class="main-content-block">
            <!-- (rest of Evaluation Metrics unchanged) -->

          <h2>Evaluation Metrics</h2>
          <p>To quantify how attention behaves, we compute metrics along four dimensions:</p>

          <ol>
            <li>
              <b>Forecasting Performance</b>: Mean Squared Error (MSE) on the validation and test sets. Useful for contextualizing whether interpretable (or degenerate) attention correlates with prediction quality.
            </li>
            <p></p>

            <li>
              <b>Attention Entropy</b>:
              <p>
                \[
                  H_i = - \sum_j a_{ij} \log(a_{ij})
                \]
              </p>
              For each head and timestep, we compute attention entropy: Low entropy indicates sharp, localized attention (e.g., seasonal lags); high entropy indicates diffuse, uninformative attention (e.g., random walk segments).
            </li>
            <p></p>

            <!-- <li>
              <b>Top-k Mass Concentration</b>
              <p>Measures how much attention mass is concentrated on a few timesteps:</p>
              <p>
                \[
                  \text{Top-}k\ \text{Mass}
                  = \sum_{j \in \text{top } k} a_{ij}
                \]
              </p>
              <p>Useful for identifying:</p>
              <ul>
                <li>collapse onto recency (j near L)</li>
                <li>collapse onto shock locations</li>
                <li>periodic clustering patterns</li>
              </ul>
            </li>
            <p></p>

            <li>
              <b>Head Specialization Index</b>
              <p>Tracks whether different heads learn distinct functions (trend, seasonality, shocks) or collapse to identical behavior. We compute:</p>
              <ul>
                <li>HS = 1 − cosine similarity between head attention vectors</li>
                <li>High HSI → diverse specialized heads</li>
                <li>Low HSI → head collapse (common in noisy data)</li>
              </ul>
            </li> -->
          </ol>
        </div>
        <div class="margin-right-block">
        </div>
      </div>



    <!-- RESULTS: INTRO + AR(1) TEXT -->
    <div class="content-margin-container" id="results">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">

		<h1>Results</h1>

        <h2>Synthetic Data</h2>

        <h3>I. Stochastic &amp; Volatile (Noise-Driven)</h3>

		<h4>(1) Heavy-Tailed AR(1)</h4>
		<p>
		\[
		X_t = \phi X_{t-1} + \epsilon_t,
		\]
		</p>
      </div>
      <div class="margin-right-block">
      </div>
    </div>

    <!-- AR(1) CURVES FIG -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">
		<!-- figure for heavy-tailed AR1 curves -->
        <img src="./images/AR1_curves.png"
             width="650px"
             style="display:block; margin:15px auto 25px auto;"/>
      </div>
      <div class="margin-right-block">
        <b>Figure 4.</b><br>
        Heavy-tailed AR(1) training curves: the model is able to memorize patterns present in the training windows, but it fails to generalize
      </div>
    </div>

    <!-- AR(1) ATTENTION + TEXT -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">

        <p>
          The HeavyTailedAR1 series (\(\phi = 0.7\), Student-t noise with \(\text{df} = 3\)) produces occasional extreme shocks that dominate the temporal structure. And our Transformer exhibits a clear mismatch between training and validation performance: the training loss decreases steadily, but the validation loss remains flat and eventually worsens. This indicates that although the model is able to memorize patterns present in the training windows, it fails to generalize.This mismatch arises because heavy-tailed shocks appear at different positions across samples; any pattern learned around a specific outlier does not transfer.
        </p>

        <p>The attention maps show this breakdown clearly. Although some heads display faint diagonal structure consistent with AR(1) lag dependence, these diagonals repeatedly collapse into bright vertical stripes at shock locations. Layer 0 already exhibits concentrated columns near the most recent timesteps. In some heads, short diagonal fragments accumulate into larger vertical columns, showing that the model repeatedly attends to the same shock across many query positions. This behavior is entirely consistent with the statistical properties of heavy-tailed AR(1): outliers propagate through the autoregressive recursion, creating long-lived effects that the model treats as salient. Yet because each sample contains shocks at different locations, these learned saliency patterns do not generalize beyond the training set. Entropy curves most clearly in L0H0, L1H1, and L1H2 reveal localized collapses, confirming that the model assigns almost all attention mass to the outlier and its autoregressive tail. As a result, the Transformer learns visually interpretable and distribution-specific attention behaviors that fail to improve forecasting accuracy on new sequences.</p>

		<!-- figure for heavy-tailed AR1 attention -->
        <img src="./images/AR1_attention.png"
             width="650px"
             style="display:block; margin:15px auto 25px auto;"/>
      </div>
      <div class="margin-right-block">
        <b>Figure 5.</b><br>
        Attention heatmaps for the heavy-tailed AR(1) model, showing diagonals repeatedly collapse into bright vertical stripes at shock locations.
      </div>
    </div>

    <!-- AR(1) ENTROPY FIG -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">
        <!-- figure for heavy-tailed AR1 attention -->
        <img src="./images/AR1_entropy.png"
             width="650px"
             style="display:block; margin:15px auto 25px auto;"/>
      </div>
      <div class="margin-right-block">
        <b>Figure 6.</b><br>
        Entropy heatmaps for the heavy-tailed AR(1) model, showing localized collapses at outlier positions.
      </div>
    </div>

    <!-- RESULTS: JUMP DIFFUSION TEXT -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">

		<h4>(2) Jump Diffusion</h4>
		<p>
		\[
		X_t = X_{t-1}
		+ \underbrace{\mathcal{N}(\mu, \sigma)}_{\text{diffusion}}
		+ \underbrace{J_t}_{\text{jump term}}
		\]
		</p>

		<p>
		With probability \(0.05\), \(J_t \sim \mathcal{N}(0, 5)\), otherwise \(J_t = 0\).<br>
		\(\mu = 0,\ \sigma = 1\).
		</p>

		<p>It's a smooth random walk with occasional large jump as there is a small possibility we add a high variance gaussian.</p>
      </div>
      <div class="margin-right-block">
      </div>
    </div>

    <!-- JUMP DATA FIG -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">
        <!-- figure for jump diffusion data -->
        <img src="./images/jump_data.png"
             width="650px"
             style="display:block; margin:15px auto 25px auto;"/>
      </div>
      <div class="margin-right-block">
        <b>Figure 7.</b><br>
        Example jump-diffusion time series: a mostly smooth random walk punctuated by occasional large jumps.
      </div>
    </div>

    <!-- JUMP CURVES FIG -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">
		<!-- figure for jump diffusion curves -->
        <img src="./images/jump_curves.png"
             width="650px"
             style="display:block; margin:15px auto 25px auto;"/>
      </div>
      <div class="margin-right-block">
        <b>Figure 8.</b><br>
        Training and validation loss curves for the jump-diffusion dataset, showing limited generalization beyond the smooth diffusion component.
      </div>
    </div>

    <!-- JUMP ATTENTION + TEXT -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">
		
    <p>The Transformer’s behavior on JumpDiffusion highlights the difficulty of forecasting a process with discontinuous shocks.Training loss drops smoothly, but validation loss improves only briefly before flattening, indicating that the model captures the predictable diffusion component while failing to generalize jump behavior.</p>
    <p> The attention maps reinforce this interpretation. Across Layer 0, all heads exhibit strong vertical attention stripes near the most recent positions.This is a hallmark of diffusion-like data as only the immediate past contains predictive signals.  However, the absence of diagonals or stable lag-selective patterns, along with faint fragmented vertical streaks, reveals that the model cannot form a consistent representation of jump events. Individual jumps in the training data appear as isolated bright columns in Layer 1, but these artifacts do not generalize because jump locations shift unpredictably across sequences.</p>
      
    <p>In Layer 0, most heads show a monotonic decrease in entropy as we move toward the most recent timesteps. In Layer 1, entropy patterns become more irregular and oscillatory. Heads L1H2 and L1H3 show sharp entropy drops at isolated positions, corresponding to jump-induced bright vertical columns in the attention maps. The JumpDiffusion experiments show that Transformers reliably learn the smooth diffusion component—evidenced by strong recency-biased attention and low-entropy focus near the window’s end, while failing to generalize jumps.</p>
		<!-- figure for jump diffusion attention -->
        <img src="./images/jump_attention.png"
             width="650px"
             style="display:block; margin:15px auto 25px auto;"/>
      </div>
      <div class="margin-right-block">
        <b>Figure 9.</b><br>
        Attention heatmaps on jump-diffusion sequences, with attention concentrated near the most recent timesteps and sporadic responses to jump events.
      </div>
    </div>

    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">
        <img src="./images/jump_entropy.png"
             width="650px"
             style="display:block; margin:15px auto 25px auto;"/>
      </div>
      <div class="margin-right-block">
        <b>Figure 10.</b><br>
        Entropy heatmaps on jump-diffusion sequences, showing low entropy near recent timesteps and sporadic drops corresponding to jump events.
      </div>
    </div>


    <!-- RESULTS: TREND BREAKS TEXT -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">

        <h3> II. Structural (shift-driven)</h3>
		<h4>(3) Trend Breaks</h4>
		<p>
		\[
		X_t = X_{t-1} + 0.1 + e_t
		\]
		</p>

		<p>Piecewise-linear time series where the slope changes at certain breakpoints, every step also has gaussian noise added.</p>
      </div>
      <div class="margin-right-block">
      </div>
    </div>

    <!-- TREND DATA FIG -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">
		<!-- figure for trendbreak data -->
        <img src="./images/trend_data.png"
             width="650px"
             style="display:block; margin:15px auto 25px auto;"/>
      </div>
      <div class="margin-right-block">
        <b>Figure 11.</b><br>
        Example TrendBreaks time series with piecewise-linear segments and abrupt slope changes.
      </div>
    </div>

    <!-- TREND CURVES FIG -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">
		<!-- figure for trendbreak curves -->
        <img src="./images/trend_curves.png"
             width="650px"
             style="display:block; margin:15px auto 25px auto;"/>
      </div>
      <div class="margin-right-block">
        <b>Figure 12.</b><br>
        Training and validation loss on TrendBreaks, where both quickly converge, reflecting the learnable piecewise-linear structure.
      </div>
    </div>

    <!-- TREND ATTENTION + TEXT -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">
		
		<p>The Transformer performs substantially better on TrendBreaks than on the stochastic or heavy-tailed generators. Training loss falls rapidly toward zero, and validation loss drops sharply before stabilizing at a low plateau, indicating that the model successfully learns the underlying structure of the dataset. This behavior reflects the nature of the TrendBreaks process: the time series consists of long, smooth linear segments with small Gaussian noise, punctuated by a small number of abrupt slope changes. While the exact locations of these breaks are unpredictable, the piecewise-linear trends themselves are highly regular and therefore easy for the model to learn. </p>
      <p>The attention maps further confirm this interpretation. Layer 0 (Heads 0–3) consistently show broad, diffuse vertical bands rather than narrow spikes. Head 1 and Head 3, in particular, place noticeable vertical mass near the rightmost 10–20 positions, indicating strong recency bias used to estimate the current slope. Layer 1 (Heads 0–3) amplifies this behavior as attention becomes denser and more textured, suggesting that the model refines its estimate of the current regime by pooling information from multiple segments. The lack of strong diagonal patterns is consistent with the piecewise-linear nature of the signal: slope estimation requires comparing values across longer horizons rather than attending to a fixed lag and these behavioral have also been confirmed by entropy graphs. Overall, the Transformer learns stable and generalizable attention patterns that align with the deterministic structure of the TrendBreaks process, yielding strong out-of-sample performance despite the unpredictability of the break locations.
</p>
    
		<!-- figure for trendbreak attention -->
        <img src="./images/trend_attention.png"
             width="650px"
             style="display:block; margin:15px auto 25px auto;"/>
      </div>
      <div class="margin-right-block">
        <b>Figure 13.</b><br>
        Attention maps on TrendBreaks, with diffuse focus over many lags and stronger weight on recent timesteps used to infer current slope.
      </div>
    </div>

    <!-- TREND ENTROPY FIG -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">
        <!-- figure for trendbreak attention -->
        <img src="./images/trend_entropy.png"
             width="650px"
             style="display:block; margin:15px auto 25px auto;"/>
      </div>
      <div class="margin-right-block">
        <b>Figure 14.</b><br>
        Entropy maps on TrendBreaks showing moderate variability without sharp collapses, consistent with broad attention patterns.
      </div>
    </div>

    <!-- RESULTS: SEASON TREND OUTLIERS TEXT -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">

        <h3>III. Pattern-driven and Deterministic</h3>
		<h4>(4) Season Trend Outliers</h4>
		<p>
		trend = 0.01&nbsp;t, <code>season_t</code> is a sinusoidal wave of amplitude 2 and frequency 0.1,
		\(\epsilon_t\) is Gaussian noise, and the large outlier term follows \(\mathcal{N}(0, 5)\).
		</p>

		<p>
		\[
		X_t = \text{trend}_t + \text{season}_t + \epsilon_t + \text{outlier}_t
		\]
		</p>
      </div>
      <div class="margin-right-block">
      </div>
    </div>

    <!-- SEASON DATA FIG -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">
		<!-- figure for seasontrend data -->
        <img src="./images/season_data.png"
             width="650px"
             style="display:block; margin:15px auto 25px auto;"/>
      </div>
      <div class="margin-right-block">
        <b>Figure 15.</b><br>
        SeasonTrendOutliers time series combining linear trend, sinusoidal seasonality, Gaussian noise, and occasional large outliers.
      </div>
    </div>

    <!-- SEASON CURVES FIG -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">
		<!-- figure for seasontrend curves -->
        <img src="./images/season_curves.png"
             width="650px"
             style="display:block; margin:15px auto 25px auto;"/>
      </div>
      <div class="margin-right-block">
        <b>Figure 16.</b><br>
        Training and validation loss for SeasonTrendOutliers, where both rapidly converge and track closely despite occasional outliers.
      </div>
    </div>

    <!-- SEASON ATTENTION + TEXT -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">
		
      <p>The Transformer performs very well on the SeasonTrendOutliers data: both training and validation loss fall rapidly and converge to nearly identical low levels, showing that the model captures the strong periodic pattern and global trend despite occasional outliers. Because the underlying process is highly regular, most of the forecasting error is irreducible noise.</p>
      <p>The attention maps confirm this stability. In Layer 0, Heads 0–3 show diffuse attention with faint vertical bands at seasonal lags (≈10 steps), indicating early recognition of repetitive cycle structure. No head collapses onto a single lag, nor do we observe the sharp spikes seen in shock-driven datasets—outliers do not distort the global periodic pattern enough to anchor attention. In Layer 1, Heads 0 and 2 amplify these periodic vertical bands, forming clearer grid-like structures characteristic of multi-cycle alignment, while Heads 1 and 3 maintain broader, more uniform patterns consistent with trend tracking.</p>
      <p>For entropy, Layer 0 heads show moderate oscillations that align with seasonal phase shifts. Layer 1 shows stronger modulation, especially L1H0 and L1H2, where entropy dips correspond to the bright seasonal bands showing these heads specialize in detecting repeated cycle structure.</p>
    
      <!-- figure for seasontrend attention -->
        <img src="./images/season_attention.png"
             width="650px"
             style="display:block; margin:15px auto 25px auto;"/>
      </div>
      <div class="margin-right-block">
        <b>Figure 17.</b><br>
        Attention maps on SeasonTrendOutliers showing broad, smooth focus with subtle bands at seasonal lags, consistent with learned periodic structure.
      </div>
    </div>

        <!-- SEASON ENTROPY FIG -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">
        <img src="./images/season_entropy.png"
             width="650px"
             style="display:block; margin:15px auto 25px auto;"/>
      </div>
      <div class="margin-right-block">
        <b>Figure 18.</b><br>
        Entropy maps on SeasonTrendOutliers showing moderate oscillations aligned with seasonal phases, indicating heads specializing in periodic structure.
      </div>
    </div>


    <!-- RESULTS: MULTI-SEASONALITY TEXT -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">

		<h4>(5) Multi Seasonality</h4>
		<p>The MultiSeasonality generator produces a signal composed of two superimposed sinusoids—a fast cycle (amplitude 3.0, frequency 0.1, phase 0) and a slower cycle (amplitude 1.5, frequency 0.05, phase π/4)—plus small Gaussian noise.</p>
      </div>
      <div class="margin-right-block">
      </div>
    </div>

    <!-- MULTI DATA FIG -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">
		<!-- figure for multiseason data -->
        <img src="./images/multi_data.png"
             width="650px"
             style="display:block; margin:15px auto 25px auto;"/>
      </div>
      <div class="margin-right-block">
        <b>Figure 19.</b><br>
        MultiSeasonality time series constructed from two overlapping sinusoidal components with different amplitudes, frequencies, and phase shifts.
      </div>
    </div>

    <!-- MULTI CURVES FIG -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">
		<!-- figure for multiseason curves -->
        <img src="./images/multi_curves.png"
             width="650px"
             style="display:block; margin:15px auto 25px auto;"/>
      </div>
      <div class="margin-right-block">
        <b>Figure 20.</b><br>
        Training and validation loss on MultiSeasonality, where both converge to low, nearly identical values due to the highly deterministic structure.
      </div>
    </div>

    <!-- MULTI ATTENTION + TEXT -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">
		
        <p>Because these oscillations are fully deterministic, the Transformer learns them quickly: both training and validation loss drop sharply in the first few epochs and converge to the same low value, showing that the model captures the underlying periodic structure almost perfectly. </p>
        <p>The attention maps reflect this directly. Many heads display grid-like or checkerboard patterns, reflecting the interaction between the short- and long-period cycles. In Layer 0, heads such as L0H0 and L0H1 form evenly spaced vertical bands aligned with the short-period cycle, while L0H2 tracks the longer cycle with broader, slower-spaced columns; L0H3 mixes both frequencies, producing faint checkerboard patterns where the two seasonalities overlap. In Layer 1 these structures are sharpened. The entropy curves mirror this specialization: heads focused on the fast cycle show strong oscillations in entropy across positions, whereas heads aligned with the slow cycle maintain smoother, higher entropy consistent with broader receptive fields. This result shows that each seasonal component induces dependencies at consistent lags, attention concentrates on regularly spaced columns—positions separated by one cycle of each sinusoid. </p>
        <!-- figure for multiseason attention -->
        <img src="./images/multi_attention.png"
             width="650px"
             style="display:block; margin:15px auto 25px auto;"/>
      </div>
      <div class="margin-right-block">
        <b>Figure 21.</b><br>
        Attention maps on MultiSeasonality forming grid-like or checkerboard patterns that align with the interacting short- and long-period cycles.
      </div>
    </div>

    <!-- MULTI ENTROPY FIG -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">
        <!-- figure for multiseason attention -->
        <img src="./images/multi_entropy.png"
             width="650px"
             style="display:block; margin:15px auto 25px auto;"/>
      </div>
      <div class="margin-right-block">
        <b>Figure 22.</b><br>
        Entropy maps on MultiSeasonality showing strong oscillations in heads focused on the fast cycle and smoother patterns in heads aligned with the slow cycle.
      </div>
    </div>

    <!-- RESULTS: REAL DATA – S&P 500 TEXT -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">

        <h2>Real World Data Result </h2>

        <h3>S&amp;P 500 Daily Returns</h3>
      </div>
      <div class="margin-right-block">
      </div>
    </div>

    <!-- SP500 DATA FIG -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">
        <!-- figure for SP500 data -->
        <img src="./images/SP500_data.png"
             width="650px"
             style="display:block; margin:15px auto 25px auto;"/>
      </div>
      <div class="margin-right-block">
        <b>Figure 23.</b><br>
        S&amp;P 500 index time series used in our experiments, illustrating noisy evolution with occasional large moves.
      </div>
    </div>

    <!-- SP500 CURVES FIG -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">
		<!-- figure for SP500 curves -->
        <img src="./images/SP500_curves.png"
             width="650px"
             style="display:block; margin:15px auto 25px auto;"/>
      </div>
      <div class="margin-right-block">
        <b>Figure 24.</b><br>
        Training and validation loss on the S&amp;P 500 dataset, with validation loss plateauing at a relatively high level.
      </div>
    </div>

    <!-- SP500 TEXT + ATTENTION/ENTROPY -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">
		
        <p>On the real S&P 500 series, the Transformer behaves similarly to how it performs on the synthetic JumpDiffusion process: training loss drops sharply, but validation loss improves only slightly before oscillating around a high plateau. </p>
        <p>This matches financial intuition: index levels evolve as a diffusion process with occasional large shocks, overlaid with softer regularities such as business-cycle phases and calendar effects. The attention maps reflect this hybrid character. Several heads show a dominant vertical stripe near the right edge of the window, mirroring the JumpDiffusion synthetic experiments and revealing a strong reliance on the most recent prices. At the same time, other heads exhibit faint, regularly spaced vertical and horizontal bands that resemble the grid-like patterns seen in the seasonal generators (SeasonTrendOutliers and MultiSeasonality) and, to a lesser extent, the segment structure of TrendBreaks. Entropy patterns reinforce this hybrid characterization. Heads with strong recency focus (L0H0, L0H2, L0H3, L1H3) show monotonic declines in entropy toward the end of the lookback window.</p>
        <p>These weak grids suggest that the model is tentatively exploiting recurring patterns in the data, but their low contrast and instability compared to the synthetic cases show that such a structure can easily be overwhelmed by noise and regime shifts. Overall, the S&P 500 looks most like a JumpDiffusion-style process with mild, noisy seasonality and trend structure, and the attention maps faithfully encode this mix of stochastic and weakly periodic behavior.</p>
        <!-- figure for SP500 attention -->
        <img src="./images/SP500_attention.png"
             width="650px"
             style="display:block; margin:15px auto 25px auto;"/>
        
		
		<!-- figure for SP500 entropy -->
        <img src="./images/SP500_entropy.png"
             width="650px"
             style="display:block; margin:15px auto 25px auto;"/>
      </div>
      <div class="margin-right-block">
        <b>Figure 25.</b><br>
        Attention maps on S&amp;P 500 daily returns (top) and corresponding attention entropy curves (bottom), showing strong recency focus with weak, noisy grid-like structure.
      </div>
    </div>

    <!-- RESULTS: REAL DATA – SOLAR TEXT -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">

        <h3>Solar Power Generation</h3>
      </div>
      <div class="margin-right-block">
      </div>
    </div>

    <!-- SOLAR DATA FIG -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">
        <!-- figure for solar data -->
        <img src="./images/solar_data.png"
             width="650px"
             style="display:block; margin:15px auto 25px auto;"/>
      </div>
      <div class="margin-right-block">
        <b>Figure 26.</b><br>
        Solar power generation time series with strong daily cycles and longer-term seasonal variation.
      </div>
    </div>

    <!-- SOLAR CURVES FIG -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">
		<!-- figure for solar curves -->
        <img src="./images/solar_curves.png"
             width="650px"
             style="display:block; margin:15px auto 25px auto;"/>
      </div>
      <div class="margin-right-block">
        <b>Figure 27.</b><br>
        Training and validation loss on the Solar dataset, both decreasing smoothly and remaining tightly aligned.
      </div>
    </div>

    <!-- SOLAR TEXT + ATTENTION/ENTROPY -->
    <div class="content-margin-container">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">
		
        <p>The Transformer trains cleanly on the Solar dataset, with training and validation loss dropping rapidly and staying tightly aligned, indicating that the underlying structure is highly learnable and generalizes well. Solar generation is dominated by strong, repeatable cycles—especially the daily irradiance pattern—so the model quickly locks onto these periodic features, and subsequent epochs primarily refine smaller weather-driven deviations. The smooth convergence reflects the regularity of the signal and the ease with which a Transformer can exploit stable seasonal structure.</p>
        <p>The attention maps show two dominant structures: vertical stripes at fixed lags (the model repeatedly attends to “same time yesterday”) and grid-like lattices created by interactions between daily periodicity, sub-daily cloud fluctuations, and slow seasonal drift—closely matching the behavior seen in MultiSeasonality. Entropy patterns mirror these structures. Heads like L0H1 and L1H1 maintain high, smooth entropy consistent with broad seasonal integration, whereas heads such as L1H3 show sharp entropy drops aligned with bright vertical bands, indicating precise phase-aligned focus. Overall, the Solar dataset exhibits clean multi-scale periodicity, and the Transformer captures it efficiently with structured, interpretable attention.</p>
        <!-- figure for SP500 attention -->
        <img src="./images/solar_attention.png"
             width="650px"
             style="display:block; margin:15px auto 25px auto;"/>
        
		
		<!-- figure for SP500 entropy -->
        <img src="./images/solar_entropy.png"
             width="650px"
             style="display:block; margin:15px auto 25px auto;"/>
      </div>
      <div class="margin-right-block">
        <b>Figure 28.</b><br>
        Attention maps on Solar daily irradiance (top) and corresponding attention entropy curves (bottom), showing clean multi-scale periodicity with structured, interpretable focus.
      </div>
    </div>


    <!-- CONCLUSION -->
    <div class="content-margin-container" id="conclusion">
      <div class="margin-left-block">
      </div>
      <div class="main-content-block">
        <h1>Conclusion</h1>

        <p>Across all synthetic and real datasets, our results support the central hypothesis: <b> Transformer attention is a natural way to capture learnable, interpretable patterns in time series data. </b>Transformer attention is meaningful only when the underlying time series contains stable, learnable structure. Heavy-tailed and jump-driven processes produce sharp, recency-dominated attention; trend- or regime-driven series yield broad, diffuse patterns; and periodic data reliably generate clean seasonal grids. These behaviors closely match the statistical properties of each generator. Real datasets reinforce this mapping—the S&P 500 resembles a JumpDiffusion with weak seasonality, while Solar aligns with MultiSeasonality—showing that insights from controlled synthetic settings transfer to real-world time series.</p>
        <p>At the same time, the project also reveals clear limitations: attention becomes unstable and uninformative in processes dominated by randomness or discontinuous shocks, and validation performance stalls precisely when attention degenerates. These failures stem from <b>inductive bias mismatches:</b> a vanilla Transformer has no mechanism to model rare jumps, rapidly shifting volatility, or long-memory processes, and its attention maps reflect these constraints rather than true temporal dependencies.</p>
        <p>These observations point to several concrete directions for future work. Architecturally, extending the study beyond a basic encoder-only Transformer to Informer-style ProbSparse attention, Autoformer-style auto-correlation, or multi-scale convolutions may improve inductive bias and robustness to noise. From a preprocessing perspective, denoising pipelines like wavelets, STL decomposition, EMA detrending, jump filtering could help isolate meaningful structure before attention is computed. Because our study focuses deliberately on univariate series to isolate distributional effects, an important next step is to investigate how these findings change in multivariate settings, where feature engineering and variable selection become domain-dependent and may meaningfully enhance forecasting performance.</p>
        <p>Overall, while Transformers can produce interpretable attention patterns when structure is present, stable interpretability requires either stable data or architectures designed to handle instability. Future work should develop models and preprocessing pipelines that preserve meaningful attention even in noisy, irregular, or shock-dominated environments.</p>
      </div>
      <div class="margin-right-block">
      </div>
    </div>

    <!-- REFERENCES (same style as original blog) -->
		<div class="content-margin-container" id="citations">
			<div class="margin-left-block">
			</div>
			<div class="main-content-block">
				<div class='citation' id="references" style="height:auto"><br>
					<span style="font-size:16px">References:</span><br><br>
					<a id="ref_1"></a>[1] <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>, Vaswani et al., 2017<br><br>
					<a id="ref_2"></a>[2] <a href="https://arxiv.org/abs/2012.07436">Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting</a>, Zhou et al., 2021<br><br>
					<a id="ref_3"></a>[3] <a href="https://arxiv.org/abs/2106.13008">Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting</a>, Wu et al., 2021<br><br>
					<a id="ref_4"></a>[4] <a href="https://arxiv.org/abs/2205.14415">Non-stationary Transformers: Rethinking the Stationarity in Time Series Forecasting</a>, Liu et al., 2022<br><br>
					<a id="ref_5"></a>[5] <a href="https://arxiv.org/abs/2205.13504">Are Transformers Effective for Time Series Forecasting?</a>, Zeng et al., 2022<br><br>
				</div>
			</div>
			<div class="margin-right-block">
				<!-- margin notes for reference block, if any -->
			</div>
		</div>

	</body>

</html>
